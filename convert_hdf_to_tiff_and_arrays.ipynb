{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from osgeo import gdal\n",
    "import glob\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source and destination directories\n",
    "input_hdf_dir = r\"D:\\VUB\\_data\\modis_terra_LAI\\hdf\"\n",
    "pp_output_dir = r\"D:\\VUB\\_data\\modis_terra_LAI\\tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample file to extract subdatasets\n",
    "fl=r\"D:\\VUB\\_data\\fldas_monthly_data\\MOD11A1.A2000055.h21v08.061.2020043121008.hdf\"\n",
    "gdal.Open(fl,gdal.GA_ReadOnly)\n",
    "\n",
    "#Get subdatasets. hdf files contain multiple subdatasets\n",
    "subdatasets = gdal.Open(fl,gdal.GA_ReadOnly).GetSubDatasets()\n",
    "subdatasets\n",
    "\n",
    "#extracting each subdataset\n",
    "subdataset_list=list()\n",
    "for i in range(len(subdatasets)):\n",
    "    #extracting the subdataset\n",
    "    sds = gdal.Open(subdatasets[i][0], gdal.GA_ReadOnly)\n",
    "    sub_dataset=subdatasets[i][0].split(':')[-1]\n",
    "    subdataset_list.append(sub_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HDF4_EOS:EOS_GRID:\"D:\\\\VUB\\\\_data\\\\fldas_monthly_data\\\\MOD11A1.A2000055.h21v02.061.2020043121022.hdf\":MODIS_Grid_Daily_1km_LST:LST_Day_1km'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdatasets[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset\n",
    "dataset = gdal.Open(fl,gdal.GA_ReadOnly)\n",
    "subdataset =  gdal.Open(dataset.GetSubDatasets()[0][0], gdal.GA_ReadOnly)\n",
    "\n",
    "# gdalwarp\n",
    "kwargs = {'format': 'GTiff', 'dstSRS': 'EPSG:4326'}\n",
    "ds = gdal.Warp(destNameOrDestDS='example.tif',srcDSOrSrcDSTab=subdataset, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert hdf files to tiffs with EPSG: 4326 CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Consult this reference: https://bpostance.github.io/posts/processing-large-spatial-datasets.md/\n",
    "\"\"\"\n",
    "def convert_hdf_tiff_epsg4326(input_hdf_dir,pp_vars, out_file):\n",
    "    \"\"\"\n",
    "    Convert a MODIS HDF file to a GeoTIFF file with EPSG:4326 projection.\n",
    "    \"\"\"\n",
    "    # Open the HDF file\n",
    "    for pp_var in pp_vars:\n",
    "        hdf_files = glob.glob(os.path.join(input_hdf_dir, f\"{pp_var}\",\"*.hdf\"))\n",
    "        for fl in hdf_files:       \n",
    "            out_file_name = os.path.basename(fl.replace('.hdf','.tif'))\n",
    "            out_file = os.path.join(pp_output_dir, f\"{pp_var}_500\",out_file_name)\n",
    "            \n",
    "            # open dataset\n",
    "            dataset = gdal.Open(fl,gdal.GA_ReadOnly)\n",
    "            #subdataset =  gdal.Open(dataset, gdal.GA_ReadOnly).GetSubDatasets()[0][0]\n",
    "            \n",
    "            # gdalwarp\n",
    "            kwargs = {'format': 'GTiff', 'dstSRS': 'EPSG:4326'}\n",
    "            ds = gdal.Warp(destNameOrDestDS= out_file,srcDSOrSrcDSTab=subdataset, **kwargs)\n",
    "            del ds\n",
    "            del dataset\n",
    "            print(f\"Converted {fl} to \\n {out_file}\",end='\\r')\n",
    "        print(\"Conversion Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = r\"D:\\VUB\\_data\\fldas_monthly_data\\MODIS_2000_01_25_LE.tif\"\n",
    "out_file = r\"D:\\VUB\\_data\\fldas_monthly_data\\MODIS_2000_01_25_LE_4326.tif\"\n",
    "dataset = gdal.Open(fl,gdal.GA_ReadOnly)\n",
    "#subdataset =  gdal.Open(dataset, gdal.GA_ReadOnly).GetSubDatasets()[0][0]\n",
    "\n",
    "# gdalwarp\n",
    "kwargs = {'format': 'GTiff', 'dstSRS': 'EPSG:4326'}\n",
    "ds = gdal.Warp(destNameOrDestDS= out_file,srcDSOrSrcDSTab=dataset, **kwargs)\n",
    "del ds\n",
    "del dataset\n",
    "print(f\"Converted {fl} to \\n {out_file}\",end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For multiple subdatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hdf_tiff_epsg4326(input_hdf_dir, pp_vars, output_dir):\n",
    "    \"\"\"\n",
    "    Convert MODIS HDF files to GeoTIFF files with EPSG:4326 projection.\n",
    "    \n",
    "    Args:\n",
    "        input_hdf_dir (str): Directory containing the HDF files.\n",
    "        pp_vars (list): List of product variables (e.g., ['MOD11A1', 'MOD13Q1']).\n",
    "        output_dir (str): Directory to save the output GeoTIFF files.\n",
    "    \"\"\"\n",
    "    # Loop through each product variable\n",
    "    for pp_var in pp_vars:\n",
    "        # Find all HDF files in the corresponding directory\n",
    "        hdf_files = glob.glob(os.path.join(input_hdf_dir, \"*.hdf\"))\n",
    "        #sort files\n",
    "        hdf_files.sort()\n",
    "        \n",
    "        # Loop through each HDF file\n",
    "        for fl in hdf_files[0:12]:\n",
    "            # Define output file path\n",
    "            out_file_name = os.path.basename(fl).replace('.hdf', '.tif')\n",
    "            out_file = os.path.join(output_dir, f\"{pp_var}\", out_file_name)\n",
    "            \n",
    "            # Create output directory if it doesn't exist\n",
    "            os.makedirs(os.path.dirname(out_file), exist_ok=True)\n",
    "            \n",
    "            # Open the HDF file\n",
    "            dataset = gdal.Open(fl, gdal.GA_ReadOnly)\n",
    "            if dataset is None:\n",
    "                print(f\"Failed to open {fl}\")\n",
    "                continue\n",
    "            \n",
    "            # Get the subdataset according to the product variable\n",
    "            subdataset = dataset.GetSubDatasets()[1][0]\n",
    "            if not subdataset:\n",
    "                print(f\"No subdatasets found in {fl}\")\n",
    "                continue\n",
    "            try:\n",
    "            # Use subprocess to call gdal_translate command on the subdataset with EPSG:4326 and LZW compression\n",
    "                subprocess.run(\n",
    "                    [\n",
    "                        \"gdal_translate\", \n",
    "                        \"-of\", \"GTiff\", \n",
    "                         # Set projection to EPSG:4326\n",
    "                        \"-a_srs\", \"EPSG:4326\",\n",
    "                        \"-co\", \"COMPRESS=LZW\",  # Set LZW compression\n",
    "                        subdataset, \n",
    "                        out_file\n",
    "                    ],\n",
    "                    check=True\n",
    "                )\n",
    "                print(f\"Converted {fl} to {out_file}\", end='\\r')\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Error converting {fl}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error: {e}\")\n",
    "    \n",
    "            # # Reproject and save as GeoTIFF using gdalwarp\n",
    "            # kwargs = {\n",
    "            #     'format': 'GTiff',\n",
    "            #     'dstSRS': 'EPSG:4326',\n",
    "            #     'options': ['COMPRESS=LZW'],\n",
    "            #     'outputType': gdal.GDT_Float32\n",
    "            # }\n",
    "            # ds = gdal.Warp(destNameOrDestDS=out_file, srcDSOrSrcDSTab=subdataset, dstSRS='EPSG:4326', format='GTiff', outputType=gdal.GDT_Float32, options=['COMPRESS=LZW'])          \n",
    "            # if ds:\n",
    "            #     print(f\"Converted {fl} to {out_file}\", end='\\r')\n",
    "            #     del ds  # Clean up the dataset\n",
    "            # else:\n",
    "            #     print(f\"Failed to convert {fl}\")\n",
    "            \n",
    "            del dataset  # Clean up the original dataset\n",
    "    \n",
    "    print(\"\\n Conversion Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the function\n",
    "pp_vars=['Lai_500m']\n",
    "convert_hdf_tiff_epsg4326(input_hdf_dir,pp_vars, pp_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract subset of files by date threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract the date from the filename in the format 'YYYYDDD'.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The name of the file.\n",
    "\n",
    "    Returns:\n",
    "    datetime: A datetime object representing the extracted date.\n",
    "    \"\"\"\n",
    "    # Split the filename and extract the date portion after 'A' in the 3rd part\n",
    "    date_str = filename.split('_')[2][1:]  # Assuming the date is in the 4th position and starts with 'A'\n",
    "    return datetime.strptime(date_str, '%Y%j')\n",
    "\n",
    "def filter_files_by_date(files, date_threshold):\n",
    "    \"\"\"\n",
    "    Filter files that are later than the given date threshold.\n",
    "\n",
    "    Parameters:\n",
    "    files (list): List of file paths.\n",
    "    date_threshold (datetime): The date threshold for filtering files.\n",
    "\n",
    "    Returns:\n",
    "    list: List of file paths that are later than the date threshold.\n",
    "    \"\"\"\n",
    "    filtered_files = []\n",
    "    for file in files:\n",
    "        file_date = extract_date_from_filename(os.path.basename(file))\n",
    "        if file_date > date_threshold:\n",
    "            filtered_files.append(file)\n",
    "    return filtered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(r\"D:/VUB/_data/modis_gpp_npp/gpp_500/GLASS12E01_V60*.tif\")\n",
    "subset_tiles=filter_files_by_date(files, datetime(2000, 2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge, Clip MODIS tiff tiles by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_compress_tiff_files(input_files,bbox, output_file, compression='LZW'):\n",
    "    \"\"\"\n",
    "    Merges multiple GeoTIFF files into a single GeoTIFF file and compresses it.\n",
    "\n",
    "    Args:\n",
    "        input_files (list): List of input GeoTIFF file paths.\n",
    "        output_file (str): Path to the output merged and compressed GeoTIFF file.\n",
    "        compression (str): Compression type for the output file. Default is 'DEFLATE'.\n",
    "        bbox (list): Bounding box coordinates [minx, miny, maxx, maxy]\n",
    "    \"\"\"\n",
    "    # Open the input files\n",
    "    input_datasets = [gdal.Open(file) for file in input_files]\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "            # Output file already exists            \n",
    "            print(f\"Output file {output_file} already exists. Skipping...\", end='\\r')\n",
    "            return\n",
    "\n",
    "    # Check if all files were opened successfully\n",
    "    if None in input_datasets:\n",
    "        raise ValueError(\"One or more input files could not be opened\")\n",
    "\n",
    "    # Use gdal.Warp to merge the files and apply compression\n",
    "    #clip raster to a bounding box\n",
    "    gdal.Warp(srcDSOrSrcDSTab=input_datasets, \n",
    "              destNameOrDestDS=output_file,\n",
    "               format='GTiff', \n",
    "               outputBounds=[bbox[0], bbox[1], bbox[2], bbox[3]],\n",
    "                 creationOptions=['COMPRESS={}'.format(compression)]\n",
    "                 )\n",
    "    \n",
    "    print(f\"Successfully merged and compressed files into {output_file}\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tiff_tiles HDF files\n",
    "tiff_tiles=glob.glob(r'D:\\VUB\\_data\\modis_8day_gpp_2000_2023\\tiffs\\PsNet_500m/*.tif')\n",
    "bbox=[33.8, 0.0007, 39.5, 5.0]\n",
    "output_dir = r\"D:\\VUB\\_data\\modis_8day_gpp_2000_2023\\tiffs\\subset_psnet\"\n",
    "# tiff_tiles = glob.glob(r'D:\\VUB\\_data\\modis_gpp_npp\\gpp_500/*.tif')\n",
    "\n",
    "# Generate arrays for years and days\n",
    "years = np.arange(2000, 2024)\n",
    "days = np.arange(1,367, 1)  # 8-day intervals\n",
    "\n",
    "# Merge tiff_tiles files for the day\n",
    "for year in years:\n",
    "    for day in days:\n",
    "        formatted_day = str(day).zfill(3)\n",
    "        yr = str(year)\n",
    "        files_to_merge = [file for file in tiff_tiles if f'A{yr}{formatted_day}' in file]\n",
    "        \n",
    "        if files_to_merge:\n",
    "            output_file = os.path.join(output_dir,f'MOD17A2HGF.A{yr}{formatted_day}.tif')\n",
    "\n",
    "\n",
    "            #add exception if file exists so that it is skipped and the rest are processed\n",
    "            \n",
    "            merge_and_compress_tiff_files(files_to_merge,bbox, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=r\"D:\\VUB\\_data\\modis_8day_gpp_2000_2023\\tiffs\\PsNet_500m\\MOD17A2HGF.A2023329.h21v08.061.2024020184752.tif\"\n",
    "dest=r\"D:\\VUB\\_data\\modis_8day_gpp_2000_2023\\tiffs\\PsNet_500m\\subset\\MOD17A2HGF.A2023329.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdal.Warp(srcDSOrSrcDSTab=input, \n",
    "              destNameOrDestDS=dest,\n",
    "               format='GTiff', \n",
    "               outputBounds=[bbox[0], bbox[1], bbox[2], bbox[3]],\n",
    "                 creationOptions=['COMPRESS={}'.format('LZW')]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=r\"D:\\VUB\\_data\\modis_8day_gpp_2000_2023\\tiffs\\subset_Gpp\\MOD17A2HGF.A2000177.tif\"\n",
    "basename=os.path.basename(file)\n",
    "year=os.path.splitext(basename)[0].split('.')[1][1:5]\n",
    "doy=os.path.splitext(basename)[0].split('.')[1][5:8]\n",
    "doy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge files to netCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_Gtiffs_to_NetCDF(files, output_file):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    array_list=[]\n",
    "    files.sort()\n",
    "    for file in files:\n",
    "\n",
    "        #extract the year and DOY from the filename\n",
    "        basename=os.path.basename(file)\n",
    "        year=os.path.splitext(basename)[0].split('.')[1][1:5]\n",
    "        #doy='001'\n",
    "        doy=os.path.splitext(basename)[0].split('.')[1][5:8]\n",
    "\n",
    "        #convert year and DOY to datetime\n",
    "        date = pd.to_datetime(year + doy, format='%Y%j')\n",
    "\n",
    "        #read the first file to get the dimensions\n",
    "        ds = gdal.Open(file)\n",
    "        band = ds.GetRasterBand(1)\n",
    "\n",
    "        #open as array\n",
    "        arr = band.ReadAsArray()\n",
    "\n",
    "        #get the size and coordinates\n",
    "        nlat,nlon = np.shape(arr)\n",
    "        b = ds.GetGeoTransform() #bbox, interval\n",
    "        #get the number of rows and columns and multiply by the interval, then add to the origin to get the coordinates\n",
    "        lon = np.arange(nlon)*b[1]+b[0]\n",
    "        lat = np.arange(nlat)*b[5]+b[3]\n",
    "\n",
    "        #assign the coordinates to the array\n",
    "        arr = xr.DataArray(arr,coords=[lat,lon],dims=['lat','lon'])\n",
    "\n",
    "        #assign the date to the array\n",
    "        arr = arr.expand_dims('time')\n",
    "        arr['time'] = [date]\n",
    "\n",
    "        #assign nodata value\n",
    "        arr = arr.where(arr!=band.GetNoDataValue())\n",
    "\n",
    "        #to reduce file size, convert to float32\n",
    "        arr=arr.astype('float32')\n",
    "\n",
    "        #set projection\n",
    "        arr.attrs['crs'] = 'EPSG:4326'\n",
    "\n",
    "        #assign variable properties\n",
    "        arr.attrs['units'] = 'kg*C/m^2'\n",
    "        arr.attrs['long_name'] = 'MODIS Net Photosynthesis'\n",
    "        arr.attrs['source'] = 'MODIS'\n",
    "        arr.attrs['scale'] = 0.0001\n",
    "        arr.attrs['Description'] = \"The MOD17A2HGF Version 6 Gross Primary Productivity (GPP) product is a cumulative 8-day composite of values with 500 meter (m) pixel size. \\n The data product includes information about GPP and Net Photosynthesis (PSN). The PSN band values are the GPP less the Maintenance Respiration (MR). The data product also contains a PSN Quality Control (QC) layer.\"\n",
    "                                \n",
    "\n",
    "        array_list.append(arr)\n",
    "        print(f\"Processed {file}\", end='\\r')\n",
    "\n",
    "    #concatenate the list of arrays into a single xarray dataset\n",
    "    print(f\"\\n Saving the dataset to {output_file}\")\n",
    "    npp_xr=xr.concat(array_list,dim='time')\n",
    "    npp_xr.name = 'PsNet'\n",
    "\n",
    "    #save the dataset to a netcdf file\n",
    "    npp_xr.to_netcdf(output_file)\n",
    "\n",
    "    print(f\"NetCDF file saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiffs_to_convert = glob.glob(r\"D:\\VUB\\_data\\modis_8day_gpp_2000_2023\\tiffs\\subset_psnet\\*.tif\")\n",
    "output_dir=r\"D:\\VUB\\_data\\nc_files\"\n",
    "output_file=os.path.join(output_dir,'MODIS_8day_PsNet_2000_2023.nc')\n",
    "\n",
    "merge_Gtiffs_to_NetCDF(tiffs_to_convert,output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check missing files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def get_filenames_without_extension(directory):\n",
    "    files=glob.glob(os.path.join(directory,'*.tif'))\n",
    "    return {file.split(\".\")[1] for file in files}\n",
    "\n",
    "def compare_directories(dir1, dir2):\n",
    "    files_in_dir1 = get_filenames_without_extension(dir1)\n",
    "    files_in_dir2 = get_filenames_without_extension(dir2)\n",
    "\n",
    "    missing_in_dir1 = files_in_dir2 - files_in_dir1\n",
    "    missing_in_dir2 = files_in_dir1 - files_in_dir2\n",
    "\n",
    "    return missing_in_dir1, missing_in_dir2\n",
    "\n",
    "# Replace with your actual directories\n",
    "directory1 = r'D:\\VUB\\_data\\modis_8day_gpp_2000_2023\\tiffs\\PsNet_500m'\n",
    "directory2 = r'D:\\VUB\\_data\\modis_8day_gpp_2000_2023\\tiffs\\subset_psnet'\n",
    "\n",
    "missing_in_dir1, missing_in_dir2 = compare_directories(directory1, directory2)\n",
    "\n",
    "print(\"Files missing in directory 1:\", missing_in_dir1)\n",
    "print(\"Files missing in directory 2:\", missing_in_dir2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count file numbers for each year\n",
    "directory1 = r'D:\\VUB\\_data\\modis_8day_gpp_2000_2023\\tiffs\\PsNet_500m'\n",
    "direftory2=r'D:\\VUB\\_data\\modis_8day_gpp_2000_2023\\tiffs\\subset_psnet'\n",
    "files = glob.glob(os.path.join(directory1, '*.tif'))\n",
    "files.sort()\n",
    "\n",
    "#extract doy\n",
    "doy_list = [os.path.basename(file).split('.')[1][-3:] for file in files]\n",
    "year_list = [os.path.basename(file).split('.')[1][1:5] for file in files]\n",
    "ydoy_list=[os.path.basename(file).split('.')[1][1:] for file in files]\n",
    "#count the number of files for each year\n",
    "year_count = {year: year_list.count(year) for year in year_list}\n",
    "day_count={doy: doy_list.count(doy) for doy in doy_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydoy_list.sort()\n",
    "ydoy_list_ = [ydoy for ydoy in ydoy_list if '001' in ydoy]\n",
    "\n",
    "# Finding duplicates in ydoy_list_\n",
    "duplicates = [ydoy for ydoy in set(ydoy_list_) if ydoy_list_.count(ydoy) > 1]\n",
    "duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=xr.open_dataset(output_file)\n",
    "#mask nan values\n",
    "#ds=ds.where(ds<2000,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_gpp=xr.open_dataset(r\"D:\\VUB\\_data\\modis_gpp_npp_annual\\tiff\\Gpp_500m\\clipped\\MODIS_annual_GPP_2000_2023.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 3.5))\n",
    "ds_map=ds['NPP'].sel(lat=1, lon=36.5, method='nearest')\n",
    "ds_gpp_map=ds_gpp['GPP'].sel(lat=1, lon=36.5, method='nearest')\n",
    "ds_map.plot(ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cue=ds['NPP']/ds_gpp['GPP']\n",
    "cue.attrs['variable'] = 'CUE'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 3.5))\n",
    "cue_ts=cue.sel(lat=1, lon=36.5, method='nearest')\n",
    "cue_ts.plot(ax=ax)\n",
    "plt.ylim(0.45,0.55)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multipurpose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
