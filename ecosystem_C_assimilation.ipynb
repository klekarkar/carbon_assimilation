{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs on the xesmf environment\n",
    "import esmpy\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from py_ecosystem import *\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#font\n",
    "plt.rcParams['font.family'] = 'arial'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open datasets with Dask and specify chunk sizes for parallel computing: each chunk is 100 time steps\n",
    "p_const = xr.open_dataset(r\"D:\\VUB\\_data\\mswx_ETo_inputs\\psychrometric_constant_500m.nc\")\n",
    "Rn = xr.open_dataset(r\"D:\\VUB\\_data\\mswx_ETo_inputs\\Rn_ROI_10km.nc\")\n",
    "delta = xr.open_dataset(r\"D:\\VUB\\_data\\mswx_ETo_inputs\\delta_10km.nc\")\n",
    "\n",
    "# Wind constants\n",
    "wind_const_d = xr.open_dataset(r\"D:\\VUB\\_data\\mswx_ETo_inputs\\wind_const_d_10km.nc\")\n",
    "wind_const_n = xr.open_dataset(r\"D:\\VUB\\_data\\mswx_ETo_inputs\\wind_const_n_10km.nc\")\n",
    "\n",
    "# Read MODIS ET (500m)\n",
    "modis_et = xr.open_dataset(r\"D:\\VUB\\_data\\nc_files\\MODIS_ET_2000_2023.nc\")\n",
    "\n",
    "# Soil moisture content\n",
    "# fldas_sm = xr.open_dataset(r\"D:\\VUB\\_data\\nc_files\\fldas_sm.nc\", chunks={'time': 100})\n",
    "gldas_sm = xr.open_dataset(r\"D:\\VUB\\_data\\nc_files\\gldas_SM_dataset.nc\")\n",
    "gleam_ds = xr.open_dataset(r\"D:\\VUB\\_data\\nc_files\\gleam_daily_v41_2000_2023.nc\")\n",
    "\n",
    "# GPP\n",
    "modis_gpp = xr.open_dataset(r\"D:\\VUB\\_data\\nc_files\\MODIS_8day_Gpp_2000_2023.nc\")\n",
    "#modis_psnet = xr.open_dataset(r\"D:\\VUB\\_data\\nc_files\\MODIS_8day_PsNet_2000_2023.nc\")\n",
    "glass_gpp = xr.open_dataset(r\"D:\\VUB\\_data\\nc_files\\GLASS_gpp_2000_2021.nc\")\n",
    "plmv2_gpp = xr.open_dataset(r\"D:\\VUB\\_data\\nc_files\\pml_v2_GPP_Ec.nc\")\n",
    "\n",
    "#rainfall\n",
    "mswx_pcp = xr.open_dataset(r\"D:\\VUB\\_data\\nc_files\\MSWX_daily_RF_2000_2023.nc\")\n",
    "\n",
    "#humidity\n",
    "mswx_rh = xr.open_dataset(r\"D:\\VUB\\_data\\mswx_ETo_inputs\\RH_KE_data.nc\")\n",
    "mswx_rh = mswx_rh.sel(time=slice(\"2000-01-01\", '2023-10-26'))\n",
    "#vpd\n",
    "mswx_vpd = xr.open_dataset(r\"D:\\VUB\\_data\\mswx_ETo_inputs\\VPD_10km.nc\")\n",
    "\n",
    "#temperature\n",
    "mswx_tmax  = xr.open_dataset(r\"D:\\VUB\\_data\\mswx_ETo_inputs\\Tmax_KE_data.nc\")\n",
    "mswx_tmin  = xr.open_dataset(r\"D:\\VUB\\_data\\mswx_ETo_inputs\\Tmin_KE_data.nc\")\n",
    "\n",
    "mswx_tmean = (mswx_tmax.sel(time=slice(\"2000-01-01\", '2023-10-26')) + mswx_tmin.sel(time=slice(\"2000-01-01\", '2023-10-26')))/2\n",
    "\n",
    "#modis lai\n",
    "modisLAI = xr.open_dataset(r\"D:\\VUB\\_data\\nc_files\\modis_terra_LAI_2000_2024.nc\")\n",
    "\n",
    "#co2\n",
    "noa_co2 = xr.open_dataset(r\"d:\\VUB\\_data\\nc_files\\co2_monthly_2000_2021.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mswx_pcp = xr.open_dataset(r\"D:\\VUB\\_data\\nc_files\\MSWX_daily_RF_2000_2023.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_lst = xr.open_dataset(r\"D:\\VUB\\_data\\nc_files\\modis_LST_8day_2000_2004.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mswx_tmax.close(), mswx_tmin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rescale MODIS ET and remove nodata values\n",
    "modis_et_values=modis_et.where(modis_et<32700)\n",
    "modis_et_rescaled = modis_et_values*0.1\n",
    "\n",
    "#modis_psnet_values=modis_psnet.where(modis_psnet<32700)\n",
    "#modis_psnet_rescaled = modis_psnet_values*0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regridding using xESMF\n",
    "\n",
    "> https://xesmf.readthedocs.io/en/stable/notebooks/Rectilinear_grid.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the desired resolution\n",
    "resolution = 0.027  # Degrees\n",
    "\n",
    "# Create the target grid with the new resolution\n",
    "lat_start = p_const.lat.min()\n",
    "lat_end = p_const.lat.max()\n",
    "lon_start = p_const.lon.min()\n",
    "lon_end = p_const.lon.max()\n",
    "\n",
    "ds_out = xr.Dataset(\n",
    "    {\n",
    "        \"lat\": ([\"lat\"], np.arange(lat_start, lat_end + 0.5*resolution, resolution), {\"units\": \"degrees_north\"}),\n",
    "        \"lon\": ([\"lon\"], np.arange(lon_start, lon_end + 0.5*resolution, resolution), {\"units\": \"degrees_east\"}),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets to regrid\n",
    "dataset_names = ['modis_et_out','mswx_rh_out' ,'modis_gpp_out', 'mswx_tmean_out', 'gldas_sm_out',\n",
    "                'gleam_ds_out', 'mswx_pcp_out', 'mswx_vpd_out','p_const_out', 'Rn_out',\n",
    "                'delta_out', 'wind_const_d_out', 'wind_const_n_out', 'modis_lai_out', 'modis_lst_out','noa_co2_out']\n",
    "\n",
    "regridded_datasets = {}\n",
    "\n",
    "datasets = [modis_et_rescaled, mswx_rh, modis_gpp, mswx_tmean, gldas_sm, gleam_ds, mswx_pcp, mswx_vpd,\n",
    "             p_const, Rn, delta, wind_const_d, wind_const_n, modisLAI,modis_lst, noa_co2]\n",
    "\n",
    "\n",
    "for dataset, dataset_name in zip(datasets, dataset_names):\n",
    "    regridder = xe.Regridder(dataset, ds_out, 'bilinear')\n",
    "    regridded_datasets[dataset_name] = regridder(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match data to MODIS frequency\n",
    "> If correlations are to be determined using MODIS datasets, resample the frequency of the second dataset to match the time frequency of MODIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ETo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerator\n",
    "n_ETo = (0.408 * regridded_datasets['delta_out']['air_temperature'] * regridded_datasets['Rn_out']['Rn'])\n",
    "+ (regridded_datasets['p_const_out']['psychrometric_constant']*regridded_datasets['wind_const_n_out']) \n",
    "\n",
    "n_ETo = n_ETo.rename('n_ETo')\n",
    "\n",
    "n_ETo=n_ETo.to_dataset()\n",
    "#rename the variable\n",
    "# var_name=\"\".join( list(n_ETo.keys()))\n",
    "#denominator\n",
    "d_ETo=(regridded_datasets['delta_out']['air_temperature'] + regridded_datasets['p_const_out']['psychrometric_constant'] * regridded_datasets['wind_const_d_out'])\n",
    "#rename the variable\n",
    "d_var_name=\"\".join( list(d_ETo.keys()))\n",
    "d_ETo = d_ETo.rename({d_var_name:'d_ETo'})\n",
    "\n",
    "ETo = n_ETo['n_ETo']/d_ETo['d_ETo']\n",
    "ETo = ETo.rename('ETo')\n",
    "ETo=ETo.to_dataset()\n",
    "\n",
    "#close ETo inputs\n",
    "inputs = [delta, wind_const_d, wind_const_n]\n",
    "\n",
    "for input in inputs:\n",
    "    input.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aridity and water-use efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_gpp_out_scaled = regridded_datasets['modis_gpp_out']['GPP']*0.0001\n",
    "wue = modis_gpp_out_scaled/regridded_datasets['modis_et_out']['ET']\n",
    "wue = wue.rename('WUE')\n",
    "wue=wue.to_dataset()\n",
    "#assign attributes\n",
    "wue.attrs['long_name'] = 'Water Use Efficiency'\n",
    "wue.attrs['units'] = 'kg C/m^2/mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "aridty = regridded_datasets['mswx_pcp_out']['precipitation'] / ETo['ETo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaporative Fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaporative fraction\n",
    "# evap_frac = modis_et_monthly['ET']/(Rn_monthly['Rn'] * 0.408)\n",
    "# evap_frac.name='evap_frac'\n",
    "# evap_frac=evap_frac.to_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timeseries of correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soil Moisture Regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latitudinal_mean = gleam_ssm_me.mean(dim=['lat', 'time'])\n",
    "\n",
    "# # Create the figure and axis\n",
    "# fig, ax = plt.subplots(figsize=(10, 2.0))  # Swap figsize dimensions for a vertical plot\n",
    "# ax.plot(latitudinal_mean['lon'], latitudinal_mean['SMs'], color='k', label='GLEAM RZSM')\n",
    "\n",
    "# # Set labels\n",
    "# ax.set_xlabel('lon')  # Now this is the x-axis label\n",
    "# ax.set_ylabel('sm')    # This is the y-axis label\n",
    "\n",
    "# # Add the legend\n",
    "# ax.legend(loc='upper right')\n",
    "\n",
    "# # Display the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial correlation and time lags\n",
    "\n",
    "> Ensure the time frequencies of the datasets match\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeseries Lags: Vegetation response\n",
    "\n",
    "> Seasonal Lags: Split the data according to seasonal occurence of rainfall\n",
    "> Ensure the two datasets are on the same time frequency\n",
    ">\n",
    ">This section uses a series of functions in a specific order:  \n",
    "  > \n",
    "> extract_grouped_data (output: xr.Dataset): matches the frequency of climate data to modis data  \n",
    "> extract_region_timeseries (output: pd.DataFrame): from the grouped data extract timeseries of data averaged over a region  \n",
    "> detect_season_onset (output: pd.datetime): from the region timeseries of precipitation, extract the onset of rainy seasons  \n",
    "> veg_lag_correlation: (output pd.DataFrame): use the onset date to slice the veg_df and climate_df and calculate pearson r for different lag periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the datasets to be match with the modis dataset\n",
    "modis_dataset = regridded_datasets['modis_gpp_out']\n",
    "clima_dataset = regridded_datasets['gldas_sm_out']\n",
    "\n",
    "#group clim data to match modis data\n",
    "clima_grouped = extract_grouped_data(modis_dataset, clima_dataset, agg_func='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indepedent variable\n",
    "hydr_var_name='SoilMoist_RZ_tavg'\n",
    "veg_var_name='GPP'\n",
    "\n",
    "#Investigate the effects of lagging vegetation response.\n",
    "lag=0\n",
    "\n",
    "min_lon, max_lon, min_lat, max_lat = 38, 39,1, 2\n",
    "\n",
    "#extract the time series\n",
    "veg_df=extract_region_timeseries(modis_dataset.sel(time=slice('2003-02-02','2021-12-31')), min_lon, max_lon, min_lat, max_lat)\n",
    "hydr_df=extract_region_timeseries(clima_grouped.sel(time=slice('2003-02-02','2021-12-31')), min_lon, max_lon, min_lat, max_lat)\n",
    "\n",
    "#visualize the time series\n",
    "# fig,ax=plt.subplots(figsize=(11, 2.5))\n",
    "# ax2=ax.twinx()\n",
    "# veg_df[veg_var_name].shift(lag).plot(ax=ax, color='r')\n",
    "# veg_df[veg_var_name].plot(ax=ax, color='g')\n",
    "# hydr_df[hydr_var_name].plot(ax=ax2, color='k')\n",
    "\n",
    "# print(veg_df[veg_var_name].shift(lag).corr(hydr_df[hydr_var_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rainfall: Detection of Rainy season onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function1: extract rainfall data by region\n",
    "rainfall_df_daily = extract_region_timeseries(mswx_pcp, min_lon, max_lon, min_lat, max_lat)\n",
    "rainfall_df_mon=rainfall_df_daily.resample('ME').sum()\n",
    "\n",
    "#function2: extract daily accumulations of rainfall and detect the onset of the rainy season\n",
    "daily_accumulated_rainfall, season_onset = detect_season_onset(rainfall_df_daily, threshold_rainy_days=10, cutoff_day=45)\n",
    "\n",
    "#function3: slice the vegetation and climate using the detected onset of the rainy season\n",
    "#compute the correlation between the vegetation response and hydrological variable, here soil moisture\n",
    "corr_df = veg_lag_correlation(veg_df, hydr_df, range(-9, 1), season_onset, [3, 4, 5, 6, 7], 'GPP', 'SoilMoist_RZ_tavg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregated Monthly rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_df=[]\n",
    "for yr in rainfall_df_mon.index.year.unique():\n",
    "    daily_rf_by_yr=rainfall_df_mon[rainfall_df_mon.index.year==yr]\n",
    "    rf_mon=daily_rf_by_yr.groupby(daily_rf_by_yr.index.month).sum()\n",
    "    rf_mon.index=rf_mon.rename({1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}).index\n",
    "    rf_mon=rf_mon.iloc[:,[0]]\n",
    "    rf_mon.columns=[yr]\n",
    "    monthly_df.append(rf_mon)\n",
    "\n",
    "# Create a DataFrame to store the monthly rainfall values\n",
    "monthly_rainfall_df = pd.concat(monthly_df, axis=1)\n",
    "\n",
    "short_season_months = ['Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "long_season_months = ['Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul']\n",
    "\n",
    "short_season_rf=monthly_rainfall_df.loc[short_season_months]\n",
    "long_season_rf=monthly_rainfall_df.loc[long_season_months]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance of rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the variance of the rainfall\n",
    "short_season_rf_var=short_season_rf.var(axis=0)\n",
    "long_season_rf_var=long_season_rf.var(axis=0)\n",
    "\n",
    "short_season_rf_var.mean(), long_season_rf_var.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rainfall Dispersion\n",
    "iqr_season1 = short_season_rf_var.quantile(0.75) - short_season_rf_var.quantile(0.25)\n",
    "iqr_season2 = long_season_rf_var.quantile(0.75) - long_season_rf_var.quantile(0.25)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "# Create box plot to compare the distributions\n",
    "plt.boxplot([short_season_rf_var, long_season_rf_var])\n",
    "plt.ylabel('Rainfall')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lag correlation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6.5))\n",
    "\n",
    "# Plot the correlations for each year\n",
    "for year in corr_df.columns[:-4]: # Exclude the last 4 columns (mean, std_dev, etc.)\n",
    "    plt.plot(corr_df.index, corr_df[year], lw=0.8, label=f'Year {year}')\n",
    "\n",
    "# Plot a reference year (e.g., 2020) with a different style\n",
    "plt.plot(corr_df.loc[:, 2005], ls='--', color='k', lw=1.5, label='Year 2020')\n",
    "\n",
    "# Add shading between max and min correlations\n",
    "ax.fill_between(corr_df.index, corr_df['mean_corr']-corr_df['std_dev_corr'], corr_df['mean_corr']+corr_df['std_dev_corr'], color='c', alpha=0.4, label='Range of Correlations')\n",
    "\n",
    "plt.axhline(0., color='black', linestyle='--', linewidth=1)  # Add a horizontal line at y=0\n",
    "plt.xlabel('Vegetation response lag (days)')\n",
    "plt.ylabel('Correlation')\n",
    "plt.title('Correlation vs. Lag for Rainy Seasons Across Years')\n",
    "#plt.legend(title='Year')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "#add rainfall plot as inset\n",
    "\"\"\"\n",
    "Define the position and size of the inset axes. \n",
    "This inset is located at 50% from the left and 15% from the bottom of the figure, and it occupies 30% of the figure’s width and height\n",
    "\"\"\"\n",
    "ax_inset = fig.add_axes([0.17, 0.65, 0.2, 0.2]) \n",
    "monthly_rainfall_df.plot(ax=ax_inset, lw=1.0, alpha=0.2,legend=False)\n",
    "monthly_rainfall_df[2018].plot(ax=ax_inset, lw=1.0, alpha=1.0, c='r', ls='--', label='2005')\n",
    "monthly_rainfall_df.mean(axis=1).plot(ax=ax_inset, lw=1.5, color='b', label='Mean')\n",
    "ax_inset.set_xlabel('')\n",
    "#ax_inset.set_ylabel('Rainfall (mm/month)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Figures with cartopy\n",
    "\n",
    "Ref: https://medium.com/@lubomirfranko/climate-data-visualisation-with-python-visualise-climate-data-using-cartopy-and-xarray-cf35a60ca8ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.io.shapereader import Reader\n",
    "from cartopy.feature import ShapelyFeature\n",
    "\n",
    "# Load data\n",
    "data = correlation_lagged\n",
    "\n",
    "# Create a mask for no data (values less than 0.001 or NaNs)\n",
    "no_data = (data < 0.001) | data.isnull()\n",
    "\n",
    "\n",
    "# Set up the map projection and figure\n",
    "crs = ccrs.PlateCarree()\n",
    "fig = plt.figure(figsize=(15, 8.5))\n",
    "ax = plt.axes(projection=crs, frameon=True)\n",
    "\n",
    "# Plot the correlation data with a color bar\n",
    "cbar_kwargs = {'orientation': 'horizontal', 'shrink': 0.45, \"pad\": 0.05}\n",
    "data.plot(ax=ax, transform=ccrs.PlateCarree(), cmap='bwr_r', cbar_kwargs=cbar_kwargs)\n",
    "\n",
    "# Apply hatching for no data (NaN regions)\n",
    "no_data_masked = np.ma.masked_where(~no_data, no_data)  # Mask the areas with data\n",
    "ax.contourf(data.lon, data.lat, no_data_masked, transform=ccrs.PlateCarree(), colors='gray', hatches=['////'], alpha=0)\n",
    "# Add coastlines\n",
    "ax.coastlines()\n",
    "\n",
    "# Draw gridlines and labels\n",
    "gl = ax.gridlines(crs=crs, draw_labels=True,\n",
    "                  linewidth=0.6, color='gray', alpha=0.5, linestyle='-.')\n",
    "gl.xlabel_style = {\"size\": 7}\n",
    "gl.ylabel_style = {\"size\": 7}\n",
    "\n",
    "# Add lakes, rivers, and borders\n",
    "ax.add_feature(cfeature.LAKES, edgecolor='black', linewidth=0.5)\n",
    "ax.add_feature(cfeature.RIVERS, edgecolor='dodgerblue', linewidth=0.5)\n",
    "ax.add_feature(cfeature.BORDERS.with_scale(\"50m\"), lw=0.7)\n",
    "\n",
    "# Add the shapefile for Kenya counties\n",
    "shapefile_path = r\"E:\\backup\\backup_asus_pc\\F\\GIS Data\\Counties of Kenya\\County.shp\"\n",
    "shape_feature = ShapelyFeature(Reader(shapefile_path).geometries(),\n",
    "                               crs=ccrs.PlateCarree(), edgecolor='blue', facecolor='none', linewidth=0.2, alpha=0.4)\n",
    "ax.add_feature(shape_feature)\n",
    "\n",
    "ken_lakes=r\"E:\\backup\\backup_asus_pc\\F\\GIS Data\\KEN_Lakes\\KEN_Lakes_4326.shp\"\n",
    "lake_feature = ShapelyFeature(Reader(ken_lakes).geometries(),\n",
    "                               crs=ccrs.PlateCarree(), edgecolor='k', facecolor='skyblue', linewidth=0.2, alpha=1)\n",
    "ax.add_feature(lake_feature)\n",
    "\n",
    "# Set the map extent (latitude and longitude range)\n",
    "lon_min = 35\n",
    "lon_max = 39.5\n",
    "lat_min = 0.0\n",
    "lat_max = 4.0\n",
    "ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Set the plot title\n",
    "ax.set_title('Plot', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to netcdf\n",
    "# ETo_annual.to_netcdf('ETo_annual_5km.nc')\n",
    "# ETo_monthly.to_netcdf('ETo_monthly_5km.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spearman r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_r_1d(x, y):\n",
    "    \"\"\"Compute Spearman's rank correlation for two 1D arrays.\"\"\"\n",
    "    if np.all(np.isnan(x)) or np.all(np.isnan(y)):\n",
    "        return np.nan  # Return NaN if all values are NaN\n",
    "    return spearmanr(x, y, nan_policy='omit')[0]  # Omit NaNs and compute Spearman's rho\n",
    "\n",
    "# # Apply the spearman_r_1d function over the 'time' dimension\n",
    "# rho = xr.apply_ufunc(\n",
    "#     spearman_r_1d,                      # Function to apply\n",
    "#     modis_lagged[1:],                       # First variable\n",
    "#     gldas_out_slice['SoilMoist_S_tavg'],                       # Second variable\n",
    "#     input_core_dims=[['time'], ['time']],  # Core dimensions to operate over\n",
    "#     vectorize=True,                     # Vectorize to apply over all lat/lon\n",
    "#     dask='parallelized',                # Optional: parallelize with Dask\n",
    "#     output_dtypes=[float]               # Output data type\n",
    "# )\n",
    "\n",
    "# rho.plot(cmap='bwr_r', vmin=-0.5, vmax=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract timeseries for RandomForest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define reference dataset\n",
    "modis_dataset = regridded_datasets['modis_gpp_out']\n",
    "\n",
    "#define region of extraction\n",
    "min_lon, max_lon, min_lat, max_lat = 37., 38.5, 0, 0.7\n",
    "\n",
    "#match frequency of the datasets with modis outputs: where the aggregation is a mean value\n",
    "predictors_for_mean = [regridded_datasets['gleam_ds_out'][['S','SMrz','SMs']], regridded_datasets['gldas_sm_out'],\n",
    "                       regridded_datasets['mswx_vpd_out'], regridded_datasets['mswx_tmean_out'],\n",
    "                       regridded_datasets['mswx_rh_out'], regridded_datasets['modis_lai_out'],\n",
    "                       regridded_datasets['modis_lst_out']]\n",
    "\n",
    "\n",
    "dataset_avg_names = ['gleam_ds', 'gldas_sm', 'mswx_vpd', 'mswx_tmean', 'mswx_rh', 'modisLAI', 'modis_lst']\n",
    "\n",
    "datasets_grouped_mean = {}\n",
    "for name,dataset in zip(dataset_avg_names,predictors_for_mean):\n",
    "    dataset = extract_grouped_data(modis_dataset, dataset, agg_func='mean')\n",
    "    datasets_grouped_mean[name] = dataset\n",
    "\n",
    "\"\"\"compute for sum variables \"\"\"\n",
    "#predictor datasets with aggregation by summation\n",
    "predictors_for_sum = [regridded_datasets['mswx_pcp_out'], regridded_datasets['Rn_out'], \n",
    "                      ETo, regridded_datasets['modis_gpp_out'],regridded_datasets['gleam_ds_out'][['E']]]\n",
    "\n",
    "dataset_sum_names = ['mswx_pcp', 'Rn', 'ETo', 'modis_gpp','E']\n",
    "\n",
    "\n",
    "datasets_grouped_sum = {}\n",
    "for name,dataset in zip(dataset_sum_names,predictors_for_sum):\n",
    "    dataset = extract_grouped_data(modis_dataset, dataset, agg_func='sum')\n",
    "    datasets_grouped_sum[name] = dataset\n",
    "\n",
    "#combine the datasets\n",
    "##Using ** [double star] is a shortcut that allows you to pass multiple arguments to a function directly using a dictionary\n",
    "combined_datasets = {**datasets_grouped_mean, **datasets_grouped_sum}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract timeseries of features and Label\n",
    "\n",
    "To increase the sample size, sample over various locations and combine.\n",
    "In this case, timeseries from 5 ROIs have been sampled and concatenated into a single time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each variable, extract the time series for the region of interest\n",
    "#extract 4 sub-ROIs from the region bound by the min and max lat and lon\n",
    "lats = np.arange(min_lat, max_lat, 0.04)\n",
    "lons = np.arange(min_lon, max_lon, 0.04)\n",
    "\n",
    "#extract the time series for each variable\n",
    "df_ML= list()\n",
    "\n",
    "for ds in combined_datasets:\n",
    "    #list to store the extracted dataframe for each variable for combined ROIs\n",
    "    ds_combined = list()\n",
    "\n",
    "    for i in range(len(lats)-1):\n",
    "        #extract the feature data\n",
    "        df_region=extract_region_timeseries(combined_datasets[ds], lons[i], lons[i+1], lats[i], lats[i+1])\n",
    "        \n",
    "        #xtract the Label data\n",
    "        y_WUE = extract_region_timeseries(wue,lons[i], lons[i+1], lats[i], lats[i+1])\n",
    "\n",
    "        #concatenate the dataframes\n",
    "        df_feature_label = pd.concat([df_region, y_WUE], axis = 1)\n",
    "        ds_combined.append(df_feature_label)\n",
    "\n",
    "    #for each variable, combine the feature and label for all ROIs\n",
    "    df_region_fl = pd.concat(ds_combined, axis = 0)\n",
    "    \n",
    "    #append each variable to the list of predictors\n",
    "    df_ML.append(df_region_fl)\n",
    "\n",
    "# #combine the dataframes\n",
    "combined_predictor_df = pd.concat(df_ML, axis=1)\n",
    "# combined_predictor_df = combined_predictor_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictor_df = pd.concat(df_ML, axis=1)\n",
    "# #compute bulk surface resistance (100/(0.5*LAI)). The LAI is scaled by a factor of 0.1 from the original dataset\n",
    "combined_predictor_df['bulk_resistance'] = 100/(0.5*combined_predictor_df['LAI']/10)\n",
    "\n",
    "data = combined_predictor_df.drop(columns=['SoilMoist_RZ_tavg', 'SoilMoist_S_tavg', 'LAI'])\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows where bulk_resistance is greater than mean + 4*std (outliers)\n",
    "outliers = data[data['bulk_resistance'] > data['bulk_resistance'].mean() + 8 * data['bulk_resistance'].std()]\n",
    "\n",
    "# Print the index and flag as outlier\n",
    "if not outliers.empty:\n",
    "    for index in outliers.index:\n",
    "        print(data.loc[index, 'bulk_resistance'])\n",
    "else:\n",
    "    print(\"No outliers found\")\n",
    "\n",
    "# Remove the outliers from the dataset\n",
    "data = data.drop(outliers.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include data from previous time for target variable\n",
    "data['WUE_t-1'] = data['WUE'].iloc[:,0].shift(1)\n",
    "#data['stress_deg_days'] = data['LST'] - data['air_temperature']\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Monthly data \n",
    "> Since the $CO_{2}$ data is available monthly, the rest of the features are resampled to monthly frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract Monthly $CO_{2}$ timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the co2 timeseries for each ROI\n",
    "co2_roi_list = list()\n",
    "co2_ts = regridded_datasets['noa_co2_out']['co2'].sel(lat = slice(min_lat, max_lat), lon = slice(min_lon, max_lon)).mean(dim=['lat', 'lon'])\n",
    "co2_df = co2_ts.to_dataframe().drop(columns=['level'])\n",
    "co2_df_mon = co2_df.resample('ME').mean()\n",
    "\n",
    "for i in range(len(lats)-2):\n",
    "    co2_roi_list.append(co2_df_mon)\n",
    "co2_mon_df = pd.concat(co2_roi_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_monthly = data.resample('ME').mean()\n",
    "data_ML_mon = pd.concat([data_monthly, co2_df_mon], axis=1).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multicollinearity: Variance Inflation Factor\n",
    "VIF is a statistical measure that quantifies the degree of multicollinearity for each predictor variable in a regression model. When two or more predictor variables are highly correlated, VIF assesses how much the variance of an estimated regression coefficient is inflated due to this correlation. A high VIF indicates that the variable is redundant or strongly related to other predictors, leading to instability in the regression model.  \n",
    "\n",
    "The variance inflation factor is a measure of the increase of the variance of the parameter estimates if an additional variable, given by exog_idx is added to the linear regression. It is a measure for multicollinearity of the design matrix, exog.  \n",
    "\n",
    "High correlation between predictor variables, can create instability and bias in regression models.\n",
    "\n",
    "Mathematically, VIF for a predictor variable 𝑋𝑖 is defined as:\n",
    "\n",
    "The Variance Inflation Factor (VIF) for a predictor variable  $X_i$ is calculated using the formula:\n",
    "\n",
    "\n",
    "$$ VIF(X_{i}) = \\frac{1}{1 - R_{i}^2 }$$\n",
    "\n",
    "where $R_{i}^2$ is the coefficient of determination obtained by regressing the predictor $X_{i}$ on all the other predictor variables.\n",
    "\n",
    "The way to calculate a VIF for the predictors is to create an auxilliary regression on each of them against all the other predictor variables:\n",
    "\n",
    "$$X_{i} = \\beta_{0}^* + \\beta_{1}^*X_{i+1} + \\beta_{2}^*X_{i+2} +...+ \\beta_{p-1}^*X_{p} + \\epsilon$$ \n",
    "and so on for the other predictor variables.  \n",
    "​\n",
    "By running this regression, a value of $R_{i}^2$ for the specific variable which tells how well that variable is described/explained  by movements in the other variables (how much of its information is carried by the other variables) and hence the degree of its multicollinearity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "# # Extract predictor variables (excluding the target variable)\n",
    "# X = combined_predictor_df#[['E','Rn','SMrz','VPD']]\n",
    "\n",
    "# # Calculate VIF for each predictor variable\n",
    "# vif = pd.DataFrame()\n",
    "# vif['Variable'] = X.columns\n",
    "# vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# # Display variables with VIF > 5\n",
    "# #print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data_monthly.drop(columns = ['WUE'])\n",
    "# y = data_monthly['WUE'].iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['relative_humidity'] = data['relative_humidity']/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regression\n",
    "\n",
    "GridSearchCV: Exhaustive search over specified parameter values for an estimator by implementing a fit and score method.  \n",
    "Parameters are optimized by cross-validated grid-search over a parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\"\"\"-----------------------------------Random Forest Regressor---------------------------------------------\"\"\"\n",
    "\n",
    "X = data.drop(columns = ['WUE','relative_humidity','GPP','WUE_t-1'])\n",
    "y = data['GPP']#.iloc[:,0]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [150],        # Number of trees in the forest\n",
    "    'max_depth': [35],              # Maximum depth of each tree\n",
    "    'min_samples_split': [3],        # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [2],         # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': ['sqrt', 'log2', None]  # Number of features to consider at each split\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                        # Number of cross-validation folds\n",
    "    n_jobs=-1,                   # Number of jobs to run in parallel\n",
    "    scoring='neg_mean_squared_error' # Strategy to evaluate the performance of the cross-validated model on the test set.\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the performance metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "nse = 1 - (np.sum((y_test - y_pred)**2) / np.sum((y_test - y_test.mean())**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the results vs the observed values\n",
    "plt.figure(figsize=(15, 3.7))\n",
    "plt.plot(y_pred[200:700], color='dodgerblue', lw=1.5, label='Random Forest')\n",
    "plt.plot(y_test[200:700].values, color='C1', lw=1.5, label='Observations')\n",
    "print(f\"R^2:{np.round(r2, 3)}, RMSE:{np.round(rmse, 2)}, NSE:{np.round(nse, 3)}\")\n",
    "print()\n",
    "#fill 95% confidence interval\n",
    "# plt.fill_between(np.arange(0,len(y_test[600:900])), y_pred[600:900]-2*rmse, y_pred[600:900]+2*rmse, color='gray', alpha=0.2)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the point density\n",
    "from scipy.stats import gaussian_kde\n",
    "xy = np.vstack([y_test,y_pred])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6.7, 5.7))\n",
    "ax.scatter(y_test, y_pred, c=z, s=15, cmap='jet', alpha=1.0)\n",
    "ax.plot([0, y_test.max()], [0, y_test.max()], 'm-', lw=1.5)\n",
    "ax.set_xlim(.00015, y_pred.max())\n",
    "ax.set_ylim(.00015, y_pred.max())\n",
    "\n",
    "ax.set_xlabel('Observed data')\n",
    "ax.set_ylabel('Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the feature importance\n",
    "feature_importance = best_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame(feature_importance, index=X.columns, columns=['Importance'])\n",
    "\n",
    "# Sort the values in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df.index, feature_importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAPley Values\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) values are a way to explain the output of any machine learning model. It uses a game theoretic approach that measures each player's contribution to the final outcome. In machine learning, each feature is assigned an importance value representing its contribution to the model's output.\n",
    "\n",
    "SHAP values show how each feature affects each final prediction, the significance of each feature compared to others, and the model's reliance on the interaction between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Create an explainer object\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label according to the feature labels\n",
    "shap.summary_plot(shap_values, X_test, feature_names=X.columns, plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression Tree\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Extract the first tree from the random forest\n",
    "tree = best_model.estimators_[0]\n",
    "\n",
    "#plt.figure(figsize=(15, 10), dpi=200)\n",
    "\n",
    "# plot_tree(best_model.estimators_[0], feature_names=X.columns, filled=True, impurity=False, fontsize=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the SHAP beeswarm plot\n",
    "shap.summary_plot(shap_values, X_test, feature_names=X.columns,plot_size=[10,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"VPD\", shap_values, X_test, feature_names=X.columns, interaction_index='VPD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data['SMs'], (data['E']/data['Rn']), cmap='jet', c=data['precipitation'], s=15)\n",
    "plt.ylim(-0.1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_deg_days = data['LST'] - data['air_temperature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_deg_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the theil-sen estimator\n",
    "import pymannkendall as mk\n",
    "from scipy.stats import theilslopes\n",
    "from sklearn.linear_model import TheilSenRegressor, RANSACRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_WUE_data = data['precipitation'].resample('D').sum().dropna()\n",
    "\n",
    "months = np.arange(3, 7)\n",
    "\n",
    "# #extract the data for the months of interest\n",
    "#mon_WUE_data = mon_WUE_data[mon_WUE_data.index.month.isin(months)]\n",
    "\n",
    "#mon_WUE_data = mon_WUE_data.groupby(mon_WUE_data.index.year).sum()\n",
    "\n",
    "# Calculate the Mann-Kendall test statistic and p-value\n",
    "mk_result = mk.original_test(mon_WUE_data.rolling(window=2).mean())\n",
    "print(f\"{mk_result.trend}, p-value:, {mk_result.p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the slope line\n",
    "slope_line = mk_result.slope * np.arange(len(mon_WUE_data)) + mk_result.intercept\n",
    "\n",
    "# Convert slope line to a DataFrame with the same index as data\n",
    "slope_line_df = pd.DataFrame({'slope_line': slope_line}, index=mon_WUE_data.index)\n",
    "\n",
    "# Create the rolling mean of the data (12-month window)\n",
    "rolling_mean = mon_WUE_data.sort_index().rolling(window=3).mean()\n",
    "\n",
    "# Plot the rolling mean and the trend line\n",
    "fig, axis = plt.subplots(figsize=(15, 5))\n",
    "axis.plot(rolling_mean, color='g', label='Rolling Mean')\n",
    "axis.plot(slope_line_df, 'm-.', label='Trend', linewidth=0.8)\n",
    "axis.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each dataset, extract the long and short rainy season data\n",
    "#extract the long rainy season data\n",
    "long_rainy_season = data.loc[data.index.month.isin([3, 4, 5, ])]\n",
    "short_rainy_season = data.loc[data.index.month.isin([10, 11, 12])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupby date first since the data has repeated dates\n",
    "seasonal_data = (short_rainy_season['GPP']*short_rainy_season['VPD']/short_rainy_season['E']).groupby(short_rainy_season.index).mean()\n",
    "\n",
    "#calculate seasonal mean/sum of variable\n",
    "seasonal_data = seasonal_data.resample('YE').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse the trends in the WUE for the long and short rainy seasons\n",
    "# Calculate the Mann-Kendall test statistic and p-value for the long rainy season\n",
    "mk_result_long = mk.original_test(seasonal_data.rolling(window=1).mean())\n",
    "print(f\"{mk_result_long.trend}, p-value:, {mk_result_long.p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mk_result_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = seasonal_data\n",
    "x = np.arange(len(y))\n",
    "\n",
    "res = stats.theilslopes(y, x, 0.95)\n",
    "print(res[0], res[1], res[2]) # slope, intercept, lower and upper bound of the slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axis = plt.subplots(figsize=(15, 5))\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x, res[1] + res[0] * x, 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inherent WUE\n",
    "https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/2008GB003233\n",
    "\n",
    "$$IWUE = \\frac{GPP * VPD}{ET}\\$$  \n",
    "\n",
    "Computed at the Ecosystem Level. The dependence of IWUE* on\n",
    "environmental conditions indicates possible adaptive adjustment of ecosystem physiology\n",
    "in response to a changing environment. Under steady state environmental conditions, the rate\n",
    "of carbon assimilation (A) equals the rate of diffusion of\n",
    "CO2 molecules into the leaf, and the rate of transpiration (E)\n",
    "equals the rate of diffusion of H2O molecules out of the leaf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "iwue = regridded_datasets['modis_et_out'].resample(time = 'ME').sum()\n",
    "\n",
    "# Create a mask for cells that have NaN values along the time dimension\n",
    "valid_mask =  iwue.notnull().all(dim='time')\n",
    "\n",
    "def preprocessed_mann_kendall(dataset):\n",
    "    # Convert the input to a NumPy array\n",
    "    data = np.array(dataset)\n",
    "    \n",
    "    # Remove NaN values\n",
    "    data_nonan = data[~np.isnan(data)]\n",
    "    \n",
    "    # Check if there are enough valid data points for the trend analysis\n",
    "    if len(data_nonan) > 1:\n",
    "        try:\n",
    "            # Apply the Mann-Kendall test at the 95% confidence level\n",
    "            #Since the data is repeating, use the seasonal MK test with a period of 12 for monthly data\n",
    "            result = mk.seasonal_test(data_nonan, period=12)\n",
    "            \n",
    "            # Check if the trend is significant\n",
    "            if result.p <= 0.05:\n",
    "                # Return the Theil-Sen estimator slope\n",
    "                return result.slope\n",
    "            else:\n",
    "                return np.nan  # No significant trend\n",
    "        except Exception as e:\n",
    "            # Handle any exceptions that occur during the trend analysis\n",
    "            print(f\"Error encountered: {e}\")\n",
    "            return np.nan\n",
    "    else:\n",
    "        # Not enough valid data points\n",
    "        return np.nan\n",
    "\n",
    "mk_result = xr.apply_ufunc(\n",
    "    preprocessed_mann_kendall, iwue.where(valid_mask)['ET'][0:600],\n",
    "    input_core_dims=[['time']], \n",
    "    vectorize=True, dask='parallelized',\n",
    "    output_dtypes=[float]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mk_result.plot(cmap='bwr_r', robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mk_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "x_array_gridder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
