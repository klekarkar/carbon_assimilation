{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import time\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "#import rasterio as rio\n",
    "from osgeo import gdal\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import dotenv\n",
    "dotenv.load_dotenv(dotenv.find_dotenv()) #.env file in the same directory as this script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This script is intended to download MODIS data from the LP DAAC archive.\n",
    "> The files to be downloaded are .hdf files.  In this notebook I am trying to download MODIS ET and PET (MOD16A2GF.061). (https://e4ftl01.cr.usgs.gov/MOLT/MOD16A2GF.061/)\n",
    "> Each hdf file contains both of these variables and the files are in timestamped folders.\n",
    "> To download the data, one needs an earthdata login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reference: https://github.com/bpostance/learn_data_engineering/blob/main/earth.observation/modis/MCD64A1v061-burned-areas/00.ETL-MODIS.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earth data login credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials fo LP DAAC\n",
    "# you will require an EarthData login\n",
    "host = fr'https://e4ftl01.cr.usgs.gov/MOLT/MOD17A2HGF.061/'\n",
    "login = os.getenv('user')\n",
    "password = os.getenv('pwd')\n",
    "\n",
    "out_dir =r\"E:\\_data\\vegetation_dynamics_EA\\HDF_FILES\\MODIS_GPP\"\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list folders from which to download\n",
    "r = requests.get(host, verify=True, stream=True,auth=(login,password))\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "folders = list()\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"\\d{4}.\\d{2}.\\d{2}/\")}): #e.g. 2000.03.05/\n",
    "    folders.append(link.get('href'))\n",
    "print(f\"{len(folders)} folders found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # list files in folders\n",
    "# for f in folders[:]:\n",
    "#     file_list = list()\n",
    "#     folder_url = f\"{host}/{f}\"\n",
    "#     r = requests.get(folder_url, verify=True, stream=True,auth=(login,password))\n",
    "#     soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "#     for link in soup.findAll('a', attrs={'href': re.compile(\".hdf$\")}):\n",
    "#         file_list.append(link.get('href'))    \n",
    "# print(f\"{len(file_list)} files found in folder n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hreg = re.compile(\"h21\") # Kenya is in h21 and h22\n",
    "# vreg = re.compile(\"v0[8-9]\") # Kenya is in v08 and v09 (\"v0[8-9]\")\n",
    "# ken_files = list()\n",
    "# for fl in file_list:\n",
    "#     h = hreg.search(fl)\n",
    "#     if h:\n",
    "#         v = vreg.search(h.string)\n",
    "#         if v:\n",
    "#             ken_files.append(v.string)\n",
    "# print(f\"{len(ken_files)} tiles found covering the area of interest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since MODIS data is stored in numbered grids, select tiles corresponding to areas of interest.  \n",
    "> Kenya lies in 4 grids: h21v08, h22v08, h21v09, h22v09 so select only these tiles for download.  \n",
    "> https://modis-land.gsfc.nasa.gov/MODLAND_grid.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the list of folders\n",
    "r = requests.get(host, verify=True, stream=True, auth=(login, password))\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "folders = list()\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"\\d{4}.\\d{2}.\\d{2}/\")}): # e.g. 2000.03.05/\n",
    "    folders.append(link.get('href'))\n",
    "print(f\"{len(folders)} folders found\")\n",
    "\n",
    "# Compile regex for Kenyan tiles\n",
    "hreg = re.compile(\"h2[1]\")  # Kenya is in h21 and h22\n",
    "vreg = re.compile(\"v0[8-9]\")  # Kenya is in v08 and v09\n",
    "\n",
    "# List files in folders and filter for Kenyan tiles\n",
    "ken_files = list()\n",
    "for f in folders[:]:\n",
    "    folder_url = f\"{host}/{f}\"\n",
    "    r = requests.get(folder_url, verify=True, stream=True, auth=(login, password))\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\".hdf$\")}):\n",
    "        file_name = link.get('href')\n",
    "        full_file_path = f\"{folder_url}/{file_name}\"  # Concatenate folder and file\n",
    "        h = hreg.search(full_file_path)\n",
    "        if h:\n",
    "            v = vreg.search(h.string)\n",
    "            if v:\n",
    "                ken_files.append(full_file_path)\n",
    "                print(f\"Found Kenyan tile: {full_file_path}\", end=\"\\r\")\n",
    "\n",
    "print(\"\\n\" f\"The loop has found {len(ken_files)} tiles altogether\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions = list()\n",
    "for e, file_url in enumerate(ken_files):\n",
    "    local_filename = file_url.split('/')[-1]\n",
    "    print(f\"Downloading {local_filename}\", end='\\r')\n",
    "    # Pause every 10 files to avoid overloading the server\n",
    "    if (e + 1) % 10 == 0:\n",
    "        print(f\"Processed {e + 1} files, pausing for 5 seconds...\", end='\\r')\n",
    "        time.sleep(5)\n",
    "    try:\n",
    "        # Make the request to download the file\n",
    "        r = requests.get(file_url, verify=True, stream=True, auth=(login, password))\n",
    "        \n",
    "        # Save the file with its original name in the specified directory\n",
    "        local_path = f\"{out_dir}/{local_filename}\"\n",
    "        \n",
    "        with open(local_path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "    \n",
    "    except Exception as error:\n",
    "        print(f\"Error downloading {file_url}: {error}\")\n",
    "        exceptions.append(file_url)\n",
    "    \n",
    "print(\"\\n\" f\"Downloaded {len(ken_files) - len(exceptions)} files with {len(exceptions)} exceptions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined listing and download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1104 folders found\n",
      "Downloading MOD17A2HGF.A2023361.h22v09.061.2024021050114.hdf\n",
      "Download process completed with 0 exceptions\n"
     ]
    }
   ],
   "source": [
    "# Fetch the list of folders\n",
    "r = requests.get(host, verify=True, stream=True, auth=(login, password))\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "folders = list()\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"\\d{4}.\\d{2}.\\d{2}/\")}): # e.g. 2000.03.05/\n",
    "    folders.append(link.get('href'))\n",
    "print(f\"{len(folders)} folders found\")\n",
    "\n",
    "# Compile regex for Kenyan tiles\n",
    "hreg = re.compile(\"h2[2]\")  # Kenya is in h21 and h22\n",
    "vreg = re.compile(\"v0[9]\")  # Kenya is in v08 and v09\n",
    "\n",
    "# Initialize a list to keep track of any exceptions\n",
    "exceptions = list()\n",
    "\n",
    "# Loop through folders and process files\n",
    "for folder in folders[:]:\n",
    "    folder_url = f\"{host}/{folder}\"\n",
    "    r = requests.get(folder_url, verify=True, stream=True, auth=(login, password))\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    \n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\".hdf$\")}):\n",
    "        file_name = link.get('href')\n",
    "        full_file_path = f\"{folder_url}/{file_name}\"  # Concatenate folder and file\n",
    "        \n",
    "        h = hreg.search(full_file_path)\n",
    "        if h:\n",
    "            v = vreg.search(h.string)\n",
    "            if v:\n",
    "                local_filename = full_file_path.split('/')[-1]\n",
    "                print(f\"Downloading {local_filename}\", end='\\r')\n",
    "                \n",
    "                try:\n",
    "                    # Make the request to download the file\n",
    "                    r = requests.get(full_file_path, verify=True, stream=True, auth=(login, password))\n",
    "                    \n",
    "                    # Save the file with its original name in the specified directory\n",
    "                    local_path = f\"{out_dir}/{local_filename}\"\n",
    "                    \n",
    "                    with open(local_path, \"wb\") as f:\n",
    "                        f.write(r.content)\n",
    "                \n",
    "                except Exception as error:\n",
    "                    print(f\"Error downloading {full_file_path}: {error}\", end='\\r')\n",
    "                    exceptions.append(full_file_path)\n",
    "    \n",
    "    # Pause every 10 folders to avoid overloading the server\n",
    "    if (folders.index(folder) + 1) % 10 == 0:\n",
    "        print(f\"Processed {folders.index(folder) + 1} folders, pausing for 5 seconds...\", end='\\r')\n",
    "        time.sleep(5)\n",
    "\n",
    "print(\"\\n\" f\"Download process completed with {len(exceptions)} exceptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for missing files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file paths\n",
    "file_paths = glob.glob(f\"{out_dir}/*.hdf\")\n",
    "\n",
    "# Dictionary to hold counts of files by year and month\n",
    "file_count_by_date = defaultdict(int)\n",
    "\n",
    "# Regular expression to extract the year and day of year (DOY)\n",
    "pattern = r'\\.A(\\d{4})(\\d{3})\\.'\n",
    "\n",
    "for file_path in file_paths:\n",
    "    match = re.search(pattern, file_path)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        doy = int(match.group(2))\n",
    "        \n",
    "        # Convert DOY to month\n",
    "        date_str = f\"{year}-{doy:03d}\"\n",
    "        month = pd.to_datetime(date_str, format='%Y-%j').strftime('%Y-%m')\n",
    "        \n",
    "        # Increment count for the month\n",
    "        file_count_by_date[month] += 1\n",
    "\n",
    "# Print the results\n",
    "for month, count in sorted(file_count_by_date.items()):\n",
    "    print(f\"{month}: {count} file(s)\")\n",
    "\n",
    "# To identify missing months\n",
    "all_months = pd.date_range(start='2000-01', end='2020-12', freq='M').strftime('%Y-%m')\n",
    "missing_months = set(all_months) - set(file_count_by_date.keys())\n",
    "\n",
    "print(\"\\nMissing months:\")\n",
    "for month in sorted(missing_months):\n",
    "    print(month)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multipurpose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
